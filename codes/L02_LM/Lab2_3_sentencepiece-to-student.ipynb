{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Subword Tokenization\n","\n","In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n","\n","## Ref:\n","https://github.com/google/sentencepiece/blob/master/python"],"metadata":{"id":"iU5fRQwhEdJy"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"pI9gRZlUE80g"}},{"cell_type":"code","source":["!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n","!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"],"metadata":{"id":"1pOsV-jaW975"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"CSiDpG9WE-cT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQd7M6gLWPLN"},"outputs":[],"source":["import sentencepiece as spm\n","import io\n","import json"]},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"OifbmMIstzs8"}},{"cell_type":"code","source":["pantip_text = []\n","with open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n","    json_list = list(json_file)\n","    for json_str in json_list:\n","        result = json.loads(json_str)\n","        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n","sum([len(t) for t in pantip_text])"],"metadata":{"id":"-FnIDvb1lMuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"pra-apai-manee-ch1-50.txt\") as f:\n","  pra_apai_manee_data = f.readlines()"],"metadata":{"id":"yaaQVXZ8A0j1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum([len(t) for t in pra_apai_manee_data])"],"metadata":{"id":"LksJKc9MA5F_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\n","pantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n","\n","pam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\n","pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]"],"metadata":{"id":"RbdfkF-vAoie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run tokenizer training\n","\n","The Python wrapper provides multiple APIs for training our tokenizers\n","\n","1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n","  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n","  <br><br>\n","2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n","  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n","<br><br>\n","3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n","<br> Same as no.1\n","\n","\n"],"metadata":{"id":"BhwcH0Aot1XI"}},{"cell_type":"markdown","source":["### Unigram tokenizer\n","\n","We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."],"metadata":{"id":"c3XeFFYw-T_0"}},{"cell_type":"code","source":["## Train\n"],"metadata":{"id":"bFCfHphd15g9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q1 MCV\n","\n","How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n","'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"],"metadata":{"id":"gdXPaoW3_v2T"}},{"cell_type":"code","source":["len(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"],"metadata":{"id":"J1bO3s-z-PLb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BPE Tokenizer\n","\n","Now try training a BPE tokenizer."],"metadata":{"id":"tKkc1D-hAFxl"}},{"cell_type":"code","source":[],"metadata":{"id":"AiXj57rh-PIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q2 MCV\n","\n","How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n","'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"],"metadata":{"id":"nrQwGmL5AMXc"}},{"cell_type":"code","source":["len(bpe.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"],"metadata":{"id":"0AXuzyaN-PEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."],"metadata":{"id":"rbb6C6-IS_Ly"}},{"cell_type":"code","source":["unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n","\" | \".join(unigram_vocabs[:500])"],"metadata":{"id":"Aa9j6XrTKjyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n","\" | \".join(bpe_vocabs[:500])"],"metadata":{"id":"2TsXA0UqN5LN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### User-defined symbols\n","\n","Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n","\n","Refer to the documentation for ways to add these special tokens to your tokenizer.\n","\n","https://github.com/google/sentencepiece/blob/master/python"],"metadata":{"id":"eu6QnnRfQyFj"}},{"cell_type":"markdown","source":["## Train another tokenizer on another domain\n","\n","Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."],"metadata":{"id":"QEFOj62ZEdzT"}},{"cell_type":"code","source":["## Train\n"],"metadata":{"id":"O7-QkA1eMZFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analyse top tokens on different datasets\n","\n","Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."],"metadata":{"id":"R5WOVMbONnYv"}},{"cell_type":"code","source":[],"metadata":{"id":"wbfkGcsUrPYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### To answer\n","What are some notable differences you see between the two vocabs?\n","\n","Write your answer below."],"metadata":{"id":"Qz0GdZ-5YYM9"}},{"cell_type":"code","source":[],"metadata":{"id":"QxxYr0QLbDoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using tokenizer across domains\n","\n","One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n","\n","Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa."],"metadata":{"id":"ipjO87HPYl4N"}},{"cell_type":"markdown","source":["### Q3 MCV\n","\n","What percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี."],"metadata":{"id":"I4_6JG_l5BXh"}},{"cell_type":"code","source":[],"metadata":{"id":"3tCh1RaZrTAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q4 MCV\n","\n","What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip."],"metadata":{"id":"duaCJRO96SX1"}},{"cell_type":"code","source":[],"metadata":{"id":"axk9gOIgrTYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### To answer\n","Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"],"metadata":{"id":"yZYKuamv7-wI"}},{"cell_type":"code","source":[],"metadata":{"id":"gh9a6d7Q8ivJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The effect on language models\n","\n","Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."],"metadata":{"id":"O7j_Cc0p9-5S"}},{"cell_type":"markdown","source":["### Setup\n","We are going to reuse the code from the last assignment"],"metadata":{"id":"KiWztANvohhn"}},{"cell_type":"code","source":["!pip install lightning"],"metadata":{"id":"7pVtSbmVpwOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import lightning as L\n","from tqdm import tqdm\n","import numpy as np"],"metadata":{"id":"JMt5GzLrW4x3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","  def __init__(self, data, tokenizer, seq_len = 128):\n","\n","    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n","    flatten_token_ids = list(itertools.chain(*token_ids))\n","    encoded = torch.LongTensor(flatten_token_ids)\n","\n","    left_over = len(encoded) % seq_len\n","    encoded = encoded[:len(encoded)-left_over]\n","    self.encoded = encoded.view(-1, seq_len)\n","\n","  def __getitem__(self, idx):\n","    return self.encoded[idx]\n","\n","  def __len__(self):\n","    return len(self.encoded)"],"metadata":{"id":"0OIs_VS_oo1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTM(L.LightningModule):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n","\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.vocab_size=vocab_size\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n","                    dropout=dropout_rate, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","        self.learning_rate = learning_rate\n","        self.criterion = criterion\n","\n","    def forward(self, src):\n","        embedding = self.dropout(self.embedding(src))\n","        output, _ = self.lstm(embedding)\n","        output = self.dropout(output)\n","        prediction = self.fc(output)\n","        return prediction\n","\n","    def training_step(self, batch, batch_idx):\n","\n","        src = batch[:, :-1]\n","        target = batch[:, 1:]\n","        prediction = self(src)\n","        prediction = prediction.reshape(-1, self.vocab_size)\n","        target = target.reshape(-1)\n","        loss = self.criterion(prediction, target)\n","        self.log(\"train_loss\", loss)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx, dataloader_idx=0):\n","\n","        src = batch[:, :-1]\n","        target = batch[:, 1:]\n","        with torch.no_grad():\n","          prediction = self(src)\n","        prediction = prediction.reshape(-1, self.vocab_size)\n","        target = target.reshape(-1)\n","        loss = self.criterion(prediction, target)\n","        self.log(\"test_loss\", loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), lr=self.learning_rate)"],"metadata":{"id":"hk6vEPiMq34n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = sp_pam.get_piece_size()\n","embedding_dim = 200\n","hidden_dim = 512\n","num_layers = 3\n","dropout_rate = 0.2\n","lr = 1e-3\n","criterion = nn.CrossEntropyLoss()\n","train_batch_size = 64\n","test_batch_size = 128"],"metadata":{"id":"oKhuOygixndB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"kOtOE7mr-heY"}},{"cell_type":"markdown","source":["<a name=\"no1\"></a>\n","#### 1. Training on Pantip data with Pantip tokenizer"],"metadata":{"id":"g8-x9HiPDcpE"}},{"cell_type":"code","source":["trainer = L.Trainer(\n","    max_epochs=10,\n","    deterministic=True\n",")\n","model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n","\n","pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n","pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n","pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n","pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n","pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","trainer.fit(model, train_dataloaders=pantip_train_loader)"],"metadata":{"id":"oUv_A4MTx0Ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n","\n","print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n","print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n","print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n","print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"],"metadata":{"id":"1e-Y1_GYy65g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"no2\"></a>\n","#### 2. Training on Pantip data with Pra apai manee tokenizer"],"metadata":{"id":"7s3AmE4nDjmL"}},{"cell_type":"code","source":["trainer = L.Trainer(\n","    max_epochs=10,\n","    deterministic=True\n",")\n","model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n","\n","pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n","pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n","pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n","pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n","pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","trainer.fit(model, train_dataloaders=pantip_train_loader)"],"metadata":{"id":"vfRdW3m1Dmj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n","\n","print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n","print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n","print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n","print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"],"metadata":{"id":"xwLN1IarD3g9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### To answer\n","\n","The perplexity numbers should indicate that:\n","1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n","2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n","\n","Try and come up with some reasons for the results above. <br>\n","Hint:\n","1. think about \"general\" vocabs and domain-specific vocabs.\n","2. what do you think happens to the model when the token ids become longer."],"metadata":{"id":"NB8zqptTWcA6"}},{"cell_type":"code","source":[],"metadata":{"id":"TmHGQf2saPj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<a name=\"no3\"></a>\n","#### 3. Training on Pra apai manee data with Pantip tokenizer\n"],"metadata":{"id":"y8VPMm7pLdSl"}},{"cell_type":"code","source":["trainer = L.Trainer(\n","    max_epochs=10,\n","    deterministic=True\n",")\n","model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n","\n","pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n","pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n","pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n","pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n","pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","trainer.fit(model, train_dataloaders=pam_train_loader)"],"metadata":{"id":"oR5fp-YCLnnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n","\n","print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n","print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n","print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n","print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"],"metadata":{"id":"f_LhF7w7Lxwo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"no4\"></a>\n","#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n","\n","\n"],"metadata":{"id":"apk9crJjMLoW"}},{"cell_type":"code","source":["trainer = L.Trainer(\n","    max_epochs=10,\n","    deterministic=True\n",")\n","model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n","\n","pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n","pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n","pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n","pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n","\n","pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n","pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n","\n","trainer.fit(model, train_dataloaders=pam_train_loader)"],"metadata":{"id":"_G7GMBIKLzGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n","\n","print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n","print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n","print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n","print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"],"metadata":{"id":"9H753o_JMRFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### To answer\n","\n","The perplexity numbers should indicate that:\n","1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n","2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n","\n","Try and come up with some reasons for the results above. <br>"],"metadata":{"id":"en9Lmmj4dZ-1"}},{"cell_type":"code","source":[],"metadata":{"id":"HlE-mWSMfbv3"},"execution_count":null,"outputs":[]}]}